{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/r1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/r1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/r1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/r1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word, TextBlob\n",
    "from collections import Counter\n",
    "import six\n",
    "import textblob\n",
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lemma_textblob(raw_text):\n",
    "    \"\"\"\n",
    "    Get lemma from text\n",
    "    :param raw_text: Raw text to get lemma from\n",
    "    :type raw_text: str\n",
    "    :rtype: list of lemmas\n",
    "    \"\"\"\n",
    "    blob = TextBlob(raw_text)\n",
    "    return [Word(tag[0], tag[1]).lemma.lower() for tag in blob.pos_tags]\n",
    "\n",
    "def filter_corpus(text_corpus, words_to_keep = [], words_to_filter = [], n_most_common=0, \n",
    "                  rare_words_threshold=10, replace_rare = False):\n",
    "    \"\"\"\n",
    "    Filter out most common, rare, and user supplied words\n",
    "    :param corpus: a list of list of lemmas as a corpus\n",
    "    :param words_to_filter: a set of words to filter\n",
    "    :param n_most_common: top n most frequent words to remove\n",
    "    :param rare_words_threshold: filter out words if frequency is less than the threshold\n",
    "    :param replace_rare: if true replace rare words using \"UNK\", else remove rare words\n",
    "    :return: a list of list of lemmas as a corpus\n",
    "    \"\"\"\n",
    "    print(\"Start filtering\")\n",
    "    # remove any token that contains digit\n",
    "    text_corpus = [[word for word in sub_list if not any(c.isdigit() for c in word)]for sub_list in text_corpus]\n",
    "    all_tokens = [item for sub_list in text_corpus for item in sub_list]  # flatten the corpus list\n",
    "    \n",
    "    c = Counter(all_tokens)\n",
    "    common_words = [pair[0] for pair in c.most_common(n_most_common) if pair[0] not in words_to_keep] + [\"\"]\n",
    "    #print(\"The most %d common words:\"%(n_most_common))\n",
    "    #print(common_words)\n",
    "    #print(\"Filtered stop words:\")\n",
    "    #print(words_to_filter)\n",
    "    if replace_rare is True:\n",
    "        filtered = set(common_words + list(words_to_filter))\n",
    "        rare_words = set([k for k, v in six.iteritems(c) if v <= rare_words_threshold and k not in words_to_keep])\n",
    "        text_corpus = [['UNK' if word in rare_words else word for word in text] for text in text_corpus]\n",
    "    else:\n",
    "        rare_words = [k for k, v in six.iteritems(c) if v <= rare_words_threshold and k not in words_to_keep]\n",
    "        filtered = set(common_words + rare_words + list(words_to_filter))\n",
    "    #print(\"Rare words:\")\n",
    "    #print(rare_words)\n",
    "    texts = [[str(word) for word in text if word not in filtered] for text in text_corpus]\n",
    "    print(\"Done filtering\")\n",
    "    return texts\n",
    "\n",
    "def text_processing(additional_stop_words = [], words_to_keep = [], n_most_common=100, \n",
    "                    rare_words_threshold=100, replace_rare=False, gram='unigram', bigram=10):\n",
    "    \"\"\"\n",
    "    - load data\n",
    "    - lemmatization\n",
    "    - remove common words and rare words\n",
    "    - get n_gram tokens\n",
    "    :param additional_stop_words: tailor-made words to remove for the corpus\n",
    "    :type additional_stoop_words: list of str\n",
    "    :param n_most_common,rare_words_threshold,replace_rare: same with function filter_corpus()\n",
    "    :param gram: 'unigram', 'bigram', 'trigram', phrases length using gensim\n",
    "    :type gram: str\n",
    "    :param bigram: threshold for genism phrase\n",
    "    :type bigram: int\n",
    "    :return: clean tokenized documents\n",
    "    :rtype: list of list\n",
    "    \"\"\"\n",
    "#     print('Loading raw text data')\n",
    "#     sec_10k = pd.read_pickle('data/10k_raw.pickle')\n",
    "#     #raw = sec_10k.head().copy()\n",
    "#     #raw_documents = raw['mda_text'].tolist()\n",
    "#     raw_documents = sec_10k['mda_text'].tolist()\n",
    "#     #raw_documents = raw_documents[0:10]\n",
    "#     print('Lemmatizating')\n",
    "#     lemma_documents = [get_lemma_textblob(document) for document in raw_documents]\n",
    "#     pickle.dump(lemma_documents, file=open(\"data/lemma_documents.pickle\", 'wb'))\n",
    "    print('Loading lemmatized documents')\n",
    "    lemma_documents = pickle.load(file=open(\"data/10k_lemmatized.pickle\", 'rb'))\n",
    "    print('Cleaninng')\n",
    "    stop_words = stopwords.words('english') + additional_stop_words\n",
    "    clean_documents = filter_corpus(lemma_documents, words_to_keep, stop_words, n_most_common, rare_words_threshold, \n",
    "                                   replace_rare)\n",
    "    print('N_gram')\n",
    "    if gram == 'unigram':\n",
    "        return clean_documents\n",
    "    if gram == 'bigram':\n",
    "        bigram_transformer = gensim.models.Phrases(clean_documents, min_count=1, threshold=bigram)\n",
    "        bigram = list(bigram_transformer[clean_documents])\n",
    "        return bigram\n",
    "    if gram == 'trigram':\n",
    "        bigram_transformer = gensim.models.Phrases(clean_documents, min_count=1)\n",
    "        bigram = list(bigram_transformer[clean_documents])\n",
    "        trigram_transformer = gensim.models.Phrases(bigram, min_count=1)\n",
    "        trigram = list(trigram_transformer[bigram])\n",
    "        return trigram\n",
    "\n",
    "def similar_words(word2vec_model, dimension, n=5):\n",
    "    \"\"\"\n",
    "    given a dimension of seed word or list of seed words find most similar words in word2vec corpus, based on cosine similarity\n",
    "    :param word2vec_model: gensim word2vec model\n",
    "    :type dimension: str\n",
    "    :param n: the most n similar words\n",
    "    :type n: int\n",
    "    :rtype: list of (similar_word, similarity) tuples\n",
    "    \"\"\"\n",
    "    similar_words = []\n",
    "    for word in seed_words[dimension]:\n",
    "        if isinstance(word, list):\n",
    "            #make sure every word in seed word list is in word2vec corpus\n",
    "            updated_word = [item for item in word if item in word2vec_model.vocab]\n",
    "            try:\n",
    "                for pair in word2vec_model.most_similar(updated_word, topn=n):\n",
    "                    similar_words.append(pair)\n",
    "            except:\n",
    "                pass\n",
    "                #similar_words[', '.join(word)] = ('All the words in this seed word list not found in corpus', 0)\n",
    "        else:\n",
    "            try:\n",
    "                for pair in word2vec_model.similar_by_word(word, topn=n):\n",
    "                    similar_words.append(pair)\n",
    "            except:\n",
    "                pass\n",
    "                #similar_words[word] = ('Seed word not found in corpus', 0)\n",
    "    return similar_words\n",
    "\n",
    "def expand_seed_words(documents, fname, min_count=1, size=100, window=5, workers=16):\n",
    "    print('Building word2vec model')\n",
    "    model = gensim.models.Word2Vec(documents, min_count=min_count, size=size, window=window, workers=workers)\n",
    "    model.save(fname)\n",
    "    model.init_sims(replace=True)\n",
    "    expanded_words = {}\n",
    "    for dimension in seed_words:\n",
    "        expanded_words[dimension] = similar_words(word2vec_model=model, dimension=dimension)\n",
    "    return expanded_words\n",
    "\n",
    "def output(expanded_words_list, seed_words, n, fname):\n",
    "    combined_list = {}\n",
    "    for dimension in seed_words:\n",
    "        combined_list[dimension] = []\n",
    "    for item in expanded_words_list:\n",
    "        for k,v in item.items():\n",
    "            for expanded_word_tuple in v:\n",
    "                current_expanded = [tup[0]for tup in combined_list[k]]\n",
    "                if expanded_word_tuple[0] not in current_expanded:\n",
    "                    combined_list[k].append(expanded_word_tuple)\n",
    "    with open(fname, 'w') as text_file:\n",
    "        for k in combined_list:\n",
    "            combined_list[k].sort(key=lambda tup: tup[1], reverse=True)\n",
    "            combined_list[k] = [tup[0] for tup in combined_list[k]]\n",
    "            if len(combined_list[k]) >= n:\n",
    "                combined_list[k] = combined_list[k][0:n]\n",
    "                print(\"The dimension is: {} {} words\".format(k, n), file=text_file)\n",
    "                print(\"===========================================================================\", file=text_file)\n",
    "                print(combined_list[k], file=text_file)\n",
    "                print('\\n', file=text_file)\n",
    "            else:\n",
    "                print(\"The dimension is: {}. Less than {}words, output {}.\".format(k, n, len(combined_list[k])), file=text_file)\n",
    "                print(\"===========================================================================\", file=text_file)\n",
    "                print(combined_list[k], file=text_file)\n",
    "                print('\\n', file=text_file)\n",
    "    return combined_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct original seed words based on six culture dimensions, for every dimension, only choose words with positive loadings greater than 0.4 and update using self judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_words_oreilly = {\"adaptability\":\n",
    "[\"being_innovative\", \"be_innovative\", \"risk_taking\", \"risk_taken\",\"take_risk\", \"being_willing_to_experiment\", \"fast_moving\",\n",
    "\"being_quick_to_take_advantage_of_opportunities\", \"not_being_constrained_by_many_rules\", \"adaptability\"],\n",
    "\"integrity\":\n",
    "[\"having_integrity\", 'have_integrity', 'be_honest', \"having_high_ethical_standards\", \"being_honest\", \"respecting_individuals\",\n",
    "\"being_fair\", 'be_fair', 'be_supportive'],\n",
    "\"collaborative\":[\"working_in_collaboration_with_others\", \"being_team_oriented\", \"cooperative\", \"being_supportive\",\n",
    "\"avoiding_conflict\", 'be_supportive', 'avoid_conflict'],\n",
    "\"results_oriented\":['results_oriented', \"being_results_oriented\", \"having_high_expectations_for_performance\", \"achievement_oriented\"],\n",
    "\"customer_oriented\":['customer_oriented', \"being_customer_oriented\", \"listening_to_customers\", \"being_market_driven\", 'market_driven'],\n",
    "\"detail_oriented\":[\"paying_attention_to_detail\", \"emphasizing_quality\", 'emphasize_quality', \"being_precise\", 'be_precise', 'detail', 'precise', \n",
    "                  'detail_oriented']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_words = {\"integrity\":[\"integrity\", \"ethics\", \"accountability\", \"trust\", \"honesty\", \n",
    "                           \"responsibility\", \"fairness\", \"transparency\", \n",
    "                           \"ownership\", \"fair\", \"honest\", \n",
    "                           \"ethical\", \"transparent\", \n",
    "                          ['integrity', 'ethics', 'accountability', 'honesty', 'honest', 'ethical', 'trust'], \n",
    "                          ['transparency', 'transparent', 'fair', 'fairness']],\n",
    "\"teamwork\":[\"teamwork\", \"collaboration\", \"cooperation\", \"collaborative\", \"cooperative\", \n",
    "           ['teamwork', 'collaboration', 'collaborative', 'cooperative']],\n",
    "\"innovation\":[\"innovation\", \"creativity\", \"excellence\", \"improvement\", \"passion\", \n",
    "              \"pride\", \"leadership\", \"growth\", \"performance\", \"efficiency\", \n",
    "              \"efficient\", \"results\", \"result\", 'innovative', 'creative', 'fast_moving', 'move_fast', 'result_oriented', \n",
    "              ['innovation', 'innovative', 'creativity', 'creative'], \n",
    "             ['leadership', 'leader'], \n",
    "             ['efficiency', 'efficient', 'results', 'result', 'performance']],\n",
    "\"respect\":[\"respect\", \"diversity\", \"inclusion\", \"development\", \n",
    "           \"talent\", \"employees\", \"employee\", \n",
    "           \"dignity\", \"empowerment\", 'respect_individual', \n",
    "          ['talent', 'employees', 'employee', 'development'], \n",
    "          ['diversity', 'respect', 'inclusion', 'empowerment']],\n",
    "\"quality\":[\"quality\", \"customer\", \"meet_needs\", 'meet_need', \n",
    "           \"commitment\", \"make_a_difference\", \"dedication\", 'detail_oriented', \n",
    "           \"value\", \"exceed_expectations\", \"exceed_expectation\", 'emphasize_quality', \n",
    "          ['quality', 'customer'], \n",
    "          ['commitment', 'dedication', 'value']],\n",
    "\"safety\":[\"safety\", \"health\", \"healthy\", \"work_life_balance\", \"flexibility\", \n",
    "         ['safety', 'health', 'healthy']],\n",
    "\"community\":[\"community\", \"environment\", \"caring\", \"citizenship\", \n",
    "            ['community', 'environment', 'caring', 'citizenship']],\n",
    "\"communication\":[\"communication\", \"openness\", \"open_mind\", 'communicate', \n",
    "                ['communication', 'openness', 'communicate']],\n",
    "\"hard_work\":[\"hard_work\", \"reward\", \"fun\", \"energy\", ['reward', 'fun', 'energy']]}\n",
    "\n",
    "bigram = {\"bigrams\":['being_innovative', 'be_innovative', 'risk_taking', 'risk_taken', 'take_risk', 'fast_moving', 'move_fast', \n",
    "          'have_integrity', 'having_integrity', 'be_honest', 'being_honest', 'respecting_individuals', 'respecting_individual', \n",
    "          'respect_individuals', 'respect_individual', 'being_fair', 'be_fair', 'be_supportive', 'avoiding_conflict', \n",
    "          'avoid_conflict', 'results_oriented', 'result_oriented', 'achievement_oriented', 'customer_oriented', \n",
    "          'market_driven', 'detail_oriented', 'emphasizing_quality', 'emphasize_quality', 'being_precise', \n",
    "          'be_precise']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_to_keep = ['empowerment', 'customer', 'value', 'honesty', 'honest', 'result', 'caring', 'openness']\n",
    "words_to_remove = ['rx', 'chmp', 'spse', '-results', 'cia', 'openness', 'and/or', 'apb', 'cpt', 'qt', 'cte', 'mkg', 'nhs', \n",
    "                  '1350', 'rep', 'iplex', 'hap-tm-', 'hap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before adding words_to_keep result\n",
    "# documents = text_processing()\n",
    "# expanded_words = expand_seed_words(documents=documents)\n",
    "# for k, v in expanded_words.items():\n",
    "#     print('Culture dimension: %s' %(k))\n",
    "#     print('=========================================================================================================')\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lemmatized documents\n",
      "Cleaninng\n",
      "Start filtering\n",
      "Done filtering\n",
      "N_gram\n",
      "Building word2vec model\n",
      "CPU times: user 3h 44min 34s, sys: 3min 20s, total: 3h 47min 55s\n",
      "Wall time: 47min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# after adding words_to_keep unigram result\n",
    "documents = text_processing(additional_stop_words=words_to_remove, words_to_keep=words_to_keep)\n",
    "expanded_words_unigram = expand_seed_words(documents=documents, fname='data/10k_unigram')\n",
    "\n",
    "pickle.dump(expanded_words_unigram, file=open(\"data/10k_expanded_unigram.pickle\", 'wb'))\n",
    "del documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lemmatized documents\n",
      "Cleaninng\n",
      "Start filtering\n",
      "Done filtering\n",
      "N_gram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word2vec model\n",
      "CPU times: user 4h 9min 9s, sys: 6min 3s, total: 4h 15min 12s\n",
      "Wall time: 1h 26min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# after adding words_to_keep bigram result phrase threshold=10\n",
    "documents = text_processing(additional_stop_words=words_to_remove, words_to_keep=words_to_keep, n_most_common=100, \n",
    "                            rare_words_threshold=100, replace_rare=False, gram='bigram', bigram=10)\n",
    "expanded_words_bigram_10 = expand_seed_words(documents=documents, fname='data/10k_bigram_10')\n",
    "\n",
    "pickle.dump(expanded_words_bigram_10, file=open(\"data/10k_expanded_bigram_10.pickle\", 'wb'))\n",
    "del documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lemmatized documents\n",
      "Cleaninng\n",
      "Start filtering\n",
      "Done filtering\n",
      "N_gram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word2vec model\n",
      "CPU times: user 3h 45min 32s, sys: 6min 55s, total: 3h 52min 27s\n",
      "Wall time: 1h 24min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# after adding words_to_keep bigram result phrase threshold=1\n",
    "documents = text_processing(additional_stop_words=words_to_remove, words_to_keep=words_to_keep, n_most_common=100, \n",
    "                    rare_words_threshold=100, replace_rare=False, gram='bigram', bigram=1)\n",
    "expanded_words_bigram_1 = expand_seed_words(documents=documents, fname='data/10k_bigram_1')\n",
    "\n",
    "pickle.dump(expanded_words_bigram_1, file=open(\"data/10k_expanded_bigram_1.pickle\", 'wb'))\n",
    "del documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expanded_unigram_10k = pickle.load(file=open(\"data/10k_expanded_unigram.pickle\", 'rb'))\n",
    "expanded_bigram_10_10k = pickle.load(file=open(\"data/10k_expanded_bigram_10.pickle\", 'rb'))\n",
    "expanded_bigram_1_10k = pickle.load(file=open(\"data/10k_expanded_bigram_1.pickle\", 'rb'))\n",
    "#expanded_unigram_glassdoor = pickle.load(file=open(\"data/glassdoor_expanded_unigram.pickle\", 'rb'))\n",
    "#expanded_bigram_10_glassdoor = pickle.load(file=open(\"data/glassdoor_expanded_bigram_10.pickle\", 'rb'))\n",
    "#expanded_bigram_1_glassdoor = pickle.load(file=open(\"data/glassdoor_expanded_bigram_1.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expanded_words_list = [expanded_unigram_10k, expanded_bigram_10_10k, expanded_bigram_1_10k]\n",
    "result = output(expanded_words_list, seed_words, 50, 'data/10k_result_50.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final_expanded_words = {}\n",
    "# for dimension in seed_words:\n",
    "#     final_expanded_words[dimension] = {}\n",
    "# for expanded_words in expanded_words_list:\n",
    "#     for dimension in expanded_words:\n",
    "#         for one_expanded_word in expanded_words[dimension]:\n",
    "#             for k,v in one_expanded_word.items():\n",
    "#                 if isinstance(v, str):\n",
    "#                     v = [v]\n",
    "#                 if k in final_expanded_words[dimension]:\n",
    "#                     final_expanded_words[dimension][k] = set(list(final_expanded_words[dimension][k]) + v)\n",
    "#                 else:\n",
    "#                     final_expanded_words[dimension][k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
