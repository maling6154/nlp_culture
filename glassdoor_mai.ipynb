{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/r1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/r1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/r1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/r1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word, TextBlob\n",
    "from collections import Counter\n",
    "import six\n",
    "import textblob\n",
    "import gensim\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multicore():\n",
    "    pool = Pool(8) # number of processes\n",
    "    results = list(pool.map(get_lemma_textblob, corpus))\n",
    "    pool.close()\n",
    "    return(results)\n",
    "\n",
    "def get_lemma_textblob(raw_text):\n",
    "    \"\"\"\n",
    "    Get lemma from text\n",
    "    :param raw_text: Raw text to get lemma from\n",
    "    :type raw_text: str\n",
    "    :rtype: list of lemmas\n",
    "    \"\"\"\n",
    "    blob = TextBlob(raw_text)\n",
    "    return [Word(tag[0], tag[1]).lemma.lower() for tag in blob.pos_tags]\n",
    "\n",
    "def filter_corpus(text_corpus, words_to_keep = [], words_to_filter = [], n_most_common=0, \n",
    "                  rare_words_threshold=10, replace_rare = False):\n",
    "    \"\"\"\n",
    "    Filter out most common, rare, and user supplied words\n",
    "    :param corpus: a list of list of lemmas as a corpus\n",
    "    :param words_to_filter: a set of words to filter\n",
    "    :param n_most_common: top n most frequent words to remove\n",
    "    :param rare_words_threshold: filter out words if frequency is less than the threshold\n",
    "    :param replace_rare: if true replace rare words using \"UNK\", else remove rare words\n",
    "    :return: a list of list of lemmas as a corpus\n",
    "    \"\"\"\n",
    "    print(\"Start filtering\")\n",
    "    # remove any token that contains digit\n",
    "    text_corpus = [[word for word in sub_list if not any(c.isdigit() for c in word)]for sub_list in text_corpus]\n",
    "    all_tokens = [item for sub_list in text_corpus for item in sub_list]  # flatten the corpus list\n",
    "    \n",
    "    c = Counter(all_tokens)\n",
    "    common_words = [pair[0] for pair in c.most_common(n_most_common) if pair[0] not in words_to_keep] + [\"\"]\n",
    "    #print(\"The most %d common words:\"%(n_most_common))\n",
    "    #print(common_words)\n",
    "    #print(\"Filtered stop words:\")\n",
    "    #print(words_to_filter)\n",
    "    if replace_rare is True:\n",
    "        filtered = set(common_words + list(words_to_filter))\n",
    "        rare_words = set([k for k, v in six.iteritems(c) if v <= rare_words_threshold and k not in words_to_keep])\n",
    "        text_corpus = [['UNK' if word in rare_words else word for word in text] for text in text_corpus]\n",
    "    else:\n",
    "        rare_words = [k for k, v in six.iteritems(c) if v <= rare_words_threshold and k not in words_to_keep]\n",
    "        filtered = set(common_words + rare_words + list(words_to_filter))\n",
    "    #print(\"Rare words:\")\n",
    "    #print(rare_words)\n",
    "    texts = [[str(word) for word in text if word not in filtered] for text in text_corpus]\n",
    "    print(\"Done filtering\")\n",
    "    return texts\n",
    "\n",
    "def text_processing(additional_stop_words = [], words_to_keep = [], n_most_common=100, \n",
    "                    rare_words_threshold=100, replace_rare=False, gram='unigram', bigram=10):\n",
    "    \"\"\"\n",
    "    - load data\n",
    "    - lemmatization\n",
    "    - remove common words and rare words\n",
    "    - get n_gram tokens\n",
    "    :param additional_stop_words: tailor-made words to remove for the corpus\n",
    "    :type additional_stoop_words: list of str\n",
    "    :param n_most_common,rare_words_threshold,replace_rare: same with function filter_corpus()\n",
    "    :param gram: 'unigram', 'bigram', 'trigram', phrases length using gensim\n",
    "    :type gram: str\n",
    "    :param bigram: threshold for genism phrase\n",
    "    :type bigram: int\n",
    "    :return: clean tokenized documents\n",
    "    :rtype: list of list\n",
    "    \"\"\"\n",
    "#     print('Loading raw text data')\n",
    "#     sec_10k = pd.read_pickle('data/10k_raw.pickle')\n",
    "#     #raw = sec_10k.head().copy()\n",
    "#     #raw_documents = raw['mda_text'].tolist()\n",
    "#     raw_documents = sec_10k['mda_text'].tolist()\n",
    "#     #raw_documents = raw_documents[0:10]\n",
    "#     print('Lemmatizating')\n",
    "#     lemma_documents = [get_lemma_textblob(document) for document in raw_documents]\n",
    "#     pickle.dump(lemma_documents, file=open(\"data/lemma_documents.pickle\", 'wb'))\n",
    "    print('Loading lemmatized documents')\n",
    "    lemma_documents = pickle.load(file=open(\"data/glassdoor_lemmatized.pickle\", 'rb'))\n",
    "    print('Cleaninng')\n",
    "    stop_words = stopwords.words('english') + additional_stop_words\n",
    "    clean_documents = filter_corpus(lemma_documents, words_to_keep, stop_words, n_most_common, rare_words_threshold, \n",
    "                                   replace_rare)\n",
    "    print('N_gram')\n",
    "    if gram == 'unigram':\n",
    "        return clean_documents\n",
    "    if gram == 'bigram':\n",
    "        bigram_transformer = gensim.models.Phrases(clean_documents, min_count=1, threshold=bigram)\n",
    "        bigram = list(bigram_transformer[clean_documents])\n",
    "        return bigram\n",
    "    if gram == 'trigram':\n",
    "        bigram_transformer = gensim.models.Phrases(clean_documents, min_count=1)\n",
    "        bigram = list(bigram_transformer[clean_documents])\n",
    "        trigram_transformer = gensim.models.Phrases(bigram, min_count=1)\n",
    "        trigram = list(trigram_transformer[bigram])\n",
    "        return trigram\n",
    "\n",
    "def similar_words(word2vec_model, dimension, n=5):\n",
    "    \"\"\"\n",
    "    given a dimension of seed word or list of seed words find most similar words in word2vec corpus, based on cosine similarity\n",
    "    :param word2vec_model: gensim word2vec model\n",
    "    :type dimension: str\n",
    "    :param n: the most n similar words\n",
    "    :type n: int\n",
    "    :rtype: list of (similar_word, similarity) tuples\n",
    "    \"\"\"\n",
    "    similar_words = []\n",
    "    for word in seed_words[dimension]:\n",
    "        if isinstance(word, list):\n",
    "            #make sure every word in seed word list is in word2vec corpus\n",
    "            updated_word = [item for item in word if item in word2vec_model.vocab]\n",
    "            try:\n",
    "                for pair in word2vec_model.most_similar(updated_word, topn=n):\n",
    "                    similar_words.append(pair)\n",
    "            except:\n",
    "                pass\n",
    "                #similar_words[', '.join(word)] = ('All the words in this seed word list not found in corpus', 0)\n",
    "        else:\n",
    "            try:\n",
    "                for pair in word2vec_model.similar_by_word(word, topn=n):\n",
    "                    similar_words.append(pair)\n",
    "            except:\n",
    "                pass\n",
    "                #similar_words[word] = ('Seed word not found in corpus', 0)\n",
    "    return similar_words\n",
    "\n",
    "def expand_words(word2vec_model, n=50, restrict=None):\n",
    "    vocab_number = len(word2vec_model.vocab)\n",
    "    expanded_words = {}\n",
    "    if restrict != None:\n",
    "        restrict = int(vocab_number*restrict)\n",
    "    for dimension in seed_words:\n",
    "        dimension_words = [word for word in seed_words[dimension] if word in word2vec_model.vocab]\n",
    "        similar_words = [pair[0] for pair in word2vec_model.most_similar(dimension_words, topn=n, restrict_vocab=restrict)]\n",
    "        expanded_words[dimension] = similar_words\n",
    "    return expanded_words\n",
    "                \n",
    "def train_word2vec_model(documents, fname, min_count=1, size=100, window=5, workers=16):\n",
    "    print('Building word2vec model')\n",
    "    model = gensim.models.Word2Vec(documents, min_count=min_count, size=size, window=window, workers=workers)\n",
    "    model.save(fname)\n",
    "    model.init_sims(replace=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# def output(expanded_words_list, seed_words, n, fname):\n",
    "#     combined_list = {}\n",
    "#     for dimension in seed_words:\n",
    "#         combined_list[dimension] = []\n",
    "#     for item in expanded_words_list:\n",
    "#         for k,v in item.items():\n",
    "#             for expanded_word_tuple in v:\n",
    "#                 current_expanded = [tup[0]for tup in combined_list[k]]\n",
    "#                 if expanded_word_tuple[0] not in current_expanded:\n",
    "#                     combined_list[k].append(expanded_word_tuple)\n",
    "#     with open(fname, 'w') as text_file:\n",
    "#         for k in combined_list:\n",
    "#             combined_list[k].sort(key=lambda tup: tup[1], reverse=True)\n",
    "#             combined_list[k] = [tup[0] for tup in combined_list[k]]\n",
    "#             if len(combined_list[k]) >= n:\n",
    "#                 combined_list[k] = combined_list[k][0:n]\n",
    "#                 print(\"The dimension is: {} {} words\".format(k, n), file=text_file)\n",
    "#                 print(\"===========================================================================\", file=text_file)\n",
    "#                 print(combined_list[k], file=text_file)\n",
    "#                 print('\\n', file=text_file)\n",
    "#             else:\n",
    "#                 print(\"The dimension is: {}. Less than {}words, output {}.\".format(k, n, len(combined_list[k])), file=text_file)\n",
    "#                 print(\"===========================================================================\", file=text_file)\n",
    "#                 print(combined_list[k], file=text_file)\n",
    "#                 print('\\n', file=text_file)\n",
    "#     return combined_list\n",
    "\n",
    "def output_result(expanded_words, seed_words, n, fname):\n",
    "    result = {}\n",
    "    for dimension in seed_words:\n",
    "        result[dimension] = []\n",
    "    for k, v in expanded_words.items():\n",
    "        for expanded_word_tuple in v:\n",
    "            result[k].append(expanded_word_tuple)\n",
    "        result[k].sort(key=lambda tup: tup[1], reverse=True)\n",
    "    with open(fname, 'w') as text_file:\n",
    "        for k in result:\n",
    "            result[k] = [tup[0] for tup in result[k]]\n",
    "            if len(result[k]) >= n:\n",
    "                result[k] = result[k][0:n]\n",
    "                print(\"The dimension is: {} {} words\".format(k, n), file=text_file)\n",
    "                print(\"===========================================================================\", file=text_file)\n",
    "                print(result[k], file=text_file)\n",
    "                print('\\n', file=text_file)\n",
    "            else:\n",
    "                print(\"The dimension is: {}. Less than {}words, output {}.\".format(k, n, len(result[k])), file=text_file)\n",
    "                print(\"===========================================================================\", file=text_file)\n",
    "                print(result[k], file=text_file)\n",
    "                print('\\n', file=text_file)\n",
    "    return result\n",
    "\n",
    "def output_dimension_result(expanded_words, n, fname):\n",
    "    with open(fname, 'w') as text_file:\n",
    "        for k, v in expanded_words.items():\n",
    "            print(\"The dimension is: {} {} expanded words\".format(k, n), file=text_file)\n",
    "            print(\"===========================================================================\", file=text_file)\n",
    "            print(v, file=text_file)\n",
    "            print('\\n', file=text_file)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine `pros`, `cons`, `feedback` and get lemmatized reviews, saved as  `glassdoor_lemmatized.pickle`, `glassdoor_index.pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw_data = pd.read_csv('data/glassdoor_reviews_only.csv').fillna('')\n",
    "# raw_data['review'] = raw_data.pros + ' ' + raw_data.cons + ' ' + raw_data.feedback\n",
    "# corpus = raw_data['review'].tolist()\n",
    "# lemmatized = test_multicore()\n",
    "# index = raw_data['FK_reviewId'].tolist()\n",
    "# pickle.dump(lemmatized, file=open(\"data/glassdoor_lemmatized.pickle\", 'wb'))\n",
    "# pickle.dump(index, file=open('data/glassdoor_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get expanded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_words = {\"integrity\":[\"integrity\", \"ethics\", \"accountability\", \"trust\", \"honesty\", \n",
    "                           \"responsibility\", \"fairness\", \"transparency\", \n",
    "                           \"ownership\", \"fair\", \"honest\", \n",
    "                           \"ethical\", \"transparent\"],\n",
    "\"teamwork\":[\"teamwork\", \"collaboration\", \"cooperation\", \"collaborative\", \"cooperative\"],\n",
    "\"innovation\":[\"innovation\", \"creativity\", \"excellence\", \"improvement\", \"passion\", \n",
    "              \"pride\", \"leadership\", \"growth\", \"performance\", \"efficiency\", \n",
    "              \"efficient\", \"results\", \"result\", 'innovative', 'creative'],\n",
    "\"respect\":[\"respect\", \"diversity\", \"inclusion\", \"development\", \n",
    "           \"talent\", \"employees\", \"employee\", \n",
    "           \"dignity\", \"empowerment\"],\n",
    "\"quality\":[\"quality\", \"customer\", \"meet_needs\", 'meet_need', \n",
    "           \"commitment\", \"make_a_difference\", \"dedication\", \n",
    "           \"value\", \"exceed_expectations\", 'exceed_expectation'],\n",
    "\"safety\":[\"safety\", \"health\", \"healthy\", \"work_life_balance\", \"flexibility\"],\n",
    "\"community\":[\"community\", \"environment\", \"caring\", \"citizenship\"],\n",
    "\"communication\":[\"communication\", \"openness\"],\n",
    "\"hard_work\":[\"hard_work\", \"reward\", \"fun\", \"energy\"]}\n",
    "\n",
    "words_to_keep = []\n",
    "for dimension, word_list in seed_words.items():\n",
    "    for word in word_list:\n",
    "        if isinstance(word, str):\n",
    "            words_to_keep.append(word)\n",
    "words_to_remove = ['rx', 'chmp', 'spse', '-results', 'cia', 'openness', 'and/or', 'apb', 'cpt', 'qt', 'cte', 'mkg', 'nhs', \n",
    "                  '1350', 'rep', 'iplex', 'hap-tm-', 'hap']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # after adding words_to_keep unigram result\n",
    "# documents = text_processing(additional_stop_words=words_to_remove, words_to_keep=words_to_keep)\n",
    "# expanded_words_unigram = expand_seed_words(documents=documents, fname='data/glassdoor_unigram')\n",
    "\n",
    "# pickle.dump(expanded_words_unigram, file=open(\"data/glassdoor_expanded_unigram.pickle\", 'wb'))\n",
    "# del documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 768 ms, sys: 1.32 s, total: 2.08 s\n",
      "Wall time: 71.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# after adding words_to_keep bigram result phrase threshold=10\n",
    "#documents = text_processing(additional_stop_words=words_to_remove, words_to_keep=words_to_keep, n_most_common=100, \n",
    "#                            rare_words_threshold=100, replace_rare=False, gram='bigram', bigram=10)\n",
    "#model = train_word2vec_model(documents=documents, fname='data/glassdoor_bigram_10')\n",
    "\n",
    "expanded_words_100 = expand_words(word2vec_model=model, n=50, restrict=None)\n",
    "expanded_words_10 = expand_words(word2vec_model=model, n=50, restrict=0.1)\n",
    "\n",
    "output_dimension_result(expanded_words_100, fname='data/glassdoor_expanded_words_100.txt', n=50)\n",
    "output_dimension_result(expanded_words_10, fname='data/glassdoor_expanded_words_10.txt', n=50)\n",
    "#pickle.dump(expanded_words_bigram_10, file=open(\"data/glassdoor_expanded_bigram_10.pickle\", 'wb'))\n",
    "#del documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "for word in model.vocab:\n",
    "    if model.vocab[word].count in frequency:\n",
    "        frequency[model.vocab[word].count] +=1\n",
    "    else:\n",
    "        frequency[model.vocab[word].count] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "126085"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(model.vocab))\n",
    "frequency[1]+frequency[2]+frequency[3]+frequency[4]+frequency[5]+frequency[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_difference = {}\n",
    "for dimension in seed_words:\n",
    "    res_100 = set(expanded_words_100[dimension])\n",
    "    res_10 = set(expanded_words_10[dimension])\n",
    "    symmetric_difference[dimension] = res_10 ^ res_100\n",
    "output_dimension_result(symmetric_difference, fname='data/glassdoor_expanded_symmetric_difference.txt', n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # after adding words_to_keep bigram result phrase threshold=1\n",
    "# documents = text_processing(additional_stop_words=words_to_remove, words_to_keep=words_to_keep, n_most_common=100, \n",
    "#                     rare_words_threshold=100, replace_rare=False, gram='bigram', bigram=1)\n",
    "# expanded_words_bigram_1 = expand_seed_words(documents=documents, fname='data/glassdoor_bigram_1')\n",
    "\n",
    "# pickle.dump(expanded_words_bigram_1, file=open(\"data/glassdoor_expanded_bigram_1.pickle\", 'wb'))\n",
    "# del documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
